{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c55c3277",
   "metadata": {},
   "source": [
    "\n",
    "# Train & Test (SageMaker SKLearn) — **Updated full demo**\n",
    "\n",
    "This notebook trains a scikit-learn model on Amazon SageMaker using the provided `src/train_script.py`, then\n",
    "**tests predictions via Batch Transform** (cheap & no always-on endpoint).\n",
    "\n",
    "> If you still want a *real-time endpoint*, a ready-to-run cell is included but **left disabled by default** to avoid charges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbd437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Ensure required libs exist. In SageMaker Studio this is usually preinstalled.\n",
    "# Uncomment if you see import errors.\n",
    "# %pip install --upgrade sagemaker boto3 pandas scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649fa1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, boto3, time, logging, pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "# ---------- Configuration ----------\n",
    "# REQUIRED: set your S3 bucket name (no s3:// prefix). Example: \"harry-ml-demo-ca-central-1\"\n",
    "S3_BUCKET = os.getenv(\"SM_DEMO_BUCKET\", \"your-bucket\")\n",
    "# Where to store / find the CSV in the bucket (we will ensure it exists below)\n",
    "S3_DATA_KEY = \"datasets/house_prices.csv\"\n",
    "\n",
    "# Local path to the CSV in this repo (relative to this notebook in notebooks/)\n",
    "LOCAL_RELATIVE_CSV = \"../data/house_prices.csv\"\n",
    "\n",
    "# Entry script and code directory\n",
    "ENTRY_POINT = \"train_script.py\"\n",
    "SOURCE_DIR = \"../src\"\n",
    "\n",
    "# Training instance type — ml.m5.large was verified to work in ca-central-1\n",
    "TRAIN_INSTANCE_TYPE = \"ml.m5.large\"\n",
    "FRAMEWORK_VERSION = \"1.2-1\"  # SageMaker SKLearn image version\n",
    "\n",
    "# ----------------------------------\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "try:\n",
    "    role = get_execution_role()\n",
    "except Exception:\n",
    "    # Fallback for local testing (won't be used in SageMaker Studio)\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"SageMakerExecutionRole\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "print(\"Region:\", region)\n",
    "print(\"Role:\", role)\n",
    "print(\"S3 bucket:\", S3_BUCKET)\n",
    "print(\"S3 key:\", S3_DATA_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e11b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "def ensure_data_in_s3(bucket, key, local_path):\n",
    "    need_upload = False\n",
    "    try:\n",
    "        s3.head_object(Bucket=bucket, Key=key)\n",
    "        print(f\"Found existing S3 object: s3://{bucket}/{key}\")\n",
    "    except Exception as e:\n",
    "        print(\"S3 object not found, will upload local file...\", e)\n",
    "        need_upload = True\n",
    "\n",
    "    if need_upload:\n",
    "        assert os.path.exists(local_path), f\"Local file not found: {local_path}\"\n",
    "        s3_res = boto3.resource(\"s3\", region_name=region)\n",
    "        s3_res.Bucket(bucket).upload_file(local_path, key)\n",
    "        print(f\"Uploaded to s3://{bucket}/{key}\")\n",
    "\n",
    "# Run ensure step\n",
    "ensure_data_in_s3(S3_BUCKET, S3_DATA_KEY, LOCAL_RELATIVE_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a3bf63",
   "metadata": {},
   "source": [
    "\n",
    "## Train\n",
    "\n",
    "We use an **SKLearn** estimator with image version `1.2-1`. The training code lives in `../src/train_script.py`.\n",
    "Training instance uses **ml.m5.large** (verified available in ca-central-1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f5c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = SKLearn(\n",
    "    entry_point=ENTRY_POINT,\n",
    "    role=role,\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    instance_type=TRAIN_INSTANCE_TYPE,  # training\n",
    "    instance_count=1,\n",
    "    source_dir=SOURCE_DIR,\n",
    ")\n",
    "\n",
    "inputs = {'train': f's3://{S3_BUCKET}/{S3_DATA_KEY}'}\n",
    "estimator.fit(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c929265",
   "metadata": {},
   "source": [
    "\n",
    "## Test using Batch Transform (recommended for demo)\n",
    "\n",
    "This creates a one-off transform job that runs the model on a small CSV input and writes predictions to S3.\n",
    "It's cheaper than creating a real-time endpoint and avoids instance-type quirks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a4c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile, uuid, boto3, os\n",
    "\n",
    "# Create a tiny test payload (adapt format to your train_script.py expectations)\n",
    "# Example row: '1200,3'  =>  area=1200, rooms=3\n",
    "test_payload_local = os.path.join(tempfile.gettempdir(), \"bt_input.csv\")\n",
    "with open(test_payload_local, \"w\") as f:\n",
    "    f.write(\"1200,3\\n\")\n",
    "    f.write(\"1500,4\\n\")\n",
    "\n",
    "# Upload the test payload to S3\n",
    "bt_input_key = f\"batch-inputs/{uuid.uuid4().hex}/input.csv\"\n",
    "boto3.resource(\"s3\", region_name=region).Bucket(S3_BUCKET).upload_file(test_payload_local, bt_input_key)\n",
    "s3_input_uri = f\"s3://{S3_BUCKET}/{bt_input_key}\"\n",
    "print(\"Batch Transform input:\", s3_input_uri)\n",
    "\n",
    "# Create transformer\n",
    "transformer = estimator.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",  # short-lived job\n",
    "    assemble_with=\"Line\",\n",
    "    accept=\"text/csv\",\n",
    ")\n",
    "\n",
    "# Start transform\n",
    "transformer.transform(\n",
    "    data=s3_input_uri,\n",
    "    content_type=\"text/csv\",\n",
    "    split_type=\"Line\",\n",
    ")\n",
    "transformer.wait()\n",
    "\n",
    "print(\"Transform output S3:\", transformer.output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9607a35e",
   "metadata": {},
   "source": [
    "\n",
    "## (Optional) Real-time endpoint\n",
    "\n",
    "> **Not run by default** to avoid charges. If you want to try it, uncomment the cell below and run.\n",
    "> Instance types like `ml.t2.micro` are **not** supported for SKLearn in many regions; `ml.m5.large` is safe but costlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9738bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- UNCOMMENT TO DEPLOY A REAL-TIME ENDPOINT ---\n",
    "# from sagemaker.serializers import CSVSerializer\n",
    "# predictor = estimator.deploy(\n",
    "#     initial_instance_count=1,\n",
    "#     instance_type=\"ml.m5.large\",\n",
    "# )\n",
    "# predictor.serializer = CSVSerializer()\n",
    "# print(\"Endpoint name:\", predictor.endpoint_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9651a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- UNCOMMENT TO TEST THE ENDPOINT ---\n",
    "# payload = \"1200,3\"\n",
    "# result = predictor.predict(payload)\n",
    "# print(\"Prediction:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf8eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- UNCOMMENT TO DELETE THE ENDPOINT WHEN DONE ---\n",
    "# try:\n",
    "#     predictor.delete_endpoint()\n",
    "#     print(\"Endpoint deleted:\", predictor.endpoint_name)\n",
    "# except Exception as e:\n",
    "#     print(\"Nothing to delete or error deleting endpoint:\", e)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}